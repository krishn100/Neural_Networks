


# Imports
import pandas as pd
from pathlib import Path
import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler,OneHotEncoder








# Read the applicants_data.csv file from the Resources folder into a Pandas DataFrame
applicant_data_df = pd.read_csv(
    Path("applicants_data.csv")
)

# Review the DataFrame
# YOUR CODE HERE
applicant_data_df.head()


# Review the data types associated with the columns
# YOUR CODE HERE
applicant_data_df.info()





# Drop the 'EIN' and 'NAME' columns from the DataFrame
applicant_data_df = applicant_data_df.drop(['EIN', 'NAME'], axis=1)

# Review the DataFrame
# YOUR CODE HERE
applicant_data_df.head()





# Create a list of categorical variables 
categorical_variables = ["APPLICATION_TYPE", "AFFILIATION", "CLASSIFICATION", "USE_CASE","ORGANIZATION", "STATUS", "INCOME_AMT", "SPECIAL_CONSIDERATIONS", "ASK_AMT", "IS_SUCCESSFUL"]  

# Display the categorical variables list
categorical_variables


# Create a OneHotEncoder instance
enc = OneHotEncoder(sparse=False) 



# Encode the categorcal variables using OneHotEncoder
encoded_data = encoded_data = enc.fit_transform(applicant_data_df[categorical_variables])



# Create a DataFrame with the encoded variables
encoded_df = encoded_df = pd.DataFrame(
    encoded_data,
    columns = enc.get_feature_names_out(categorical_variables)
)

# Review the DataFrame
# YOUR CODE HERE
encoded_df





# Add the numerical variables from the original DataFrame to the one-hot encoding DataFrame
encoded_df = pd.concat([applicant_data_df.drop(columns=categorical_variables), encoded_df], axis=1)


# Review the Dataframe
# YOUR CODE HERE
encoded_df





# Define the target set y using the IS_SUCCESSFUL column
y = encoded_df["IS_SUCCESSFUL_1"]

# Display a sample of y
# YOUR CODE HERE
y[:5]


# Define features set X by selecting all columns but IS_SUCCESSFUL
X = encoded_df.drop(columns=["IS_SUCCESSFUL_0", "IS_SUCCESSFUL_1"])

# Review the features DataFrame
# YOUR CODE HERE
X.head()





# Split the preprocessed data into a training and testing dataset
# Assign the function a random_state equal to 1
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)






# Create a StandardScaler instance
scaler = StandardScaler()

# Fit the scaler to the features training dataset
X_scaler = scaler.fit(X_train)

# Fit the scaler to the features training dataset
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)









# Define the the number of inputs (features) to the model
number_input_features =len(X_train.iloc[0])# YOUR CODE HERE

# Review the number of features
number_input_features



# Define the number of neurons in the output layer
number_output_neurons = 1


# Define the number of hidden nodes for the first hidden layer
hidden_nodes_layer1 = 8 # YOUR CODE HERE

# Review the number hidden nodes in the first layer
hidden_nodes_layer1



# Define the number of hidden nodes for the second hidden layer
hidden_nodes_layer2 = 5 # YOUR CODE HERE

# Review the number hidden nodes in the second layer
hidden_nodes_layer2



# Create the Sequential model instance
nn = Sequential()



# Add the first hidden layer
# YOUR CODE HERE
nn.add(Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation="relu"))


# Add the second hidden layer
# YOUR CODE HERE
nn.add(Dense(units=hidden_nodes_layer2, activation="relu"))


# Add the output layer to the model specifying the number of output neurons and activation function
# YOUR CODE HERE
nn.add(Dense(units=number_output_neurons, activation="sigmoid"))


# Display the Sequential model summary
# YOUR CODE HERE
nn.summary() 





# Compile the Sequential model
# YOUR CODE HERE
nn.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])


# Fit the model using 50 epochs and the training data
# YOUR CODE HERE
fit_model = nn.fit(X_train_scaled, y_train, epochs=50)





# Evaluate the model loss and accuracy metrics using the evaluate method and the test data
model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)

# Display the model loss and accuracy results
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")





# Set the model's file path
file_path = "AlphabetSoup.h5"

# Export your model to a HDF5 file
# YOUR CODE HERE
nn.save(file_path)











# Define the the number of inputs (features) to the model
number_input_features = len(X_train.iloc[0])

# Review the number of features
number_input_features


# Define the number of neurons in the output layer
number_output_neurons_A1 = 1 # YOUR CODE HERE


# Define the number of hidden nodes for the first hidden layer
hidden_nodes_layer1_A1 = 10# YOUR CODE HERE

# Review the number of hidden nodes in the first layer
hidden_nodes_layer1_A1


# Create the Sequential model instance
nn_A1 = Sequential()# YOUR CODE HERE


# First hidden layer
# YOUR CODE HERE
nn_A1.add(Dense(units=hidden_nodes_layer1_A1, input_dim=number_input_features, activation="relu"))

# Output layer
nn_A1.add(Dense(units=number_output_neurons_A1, activation="sigmoid"))


# Check the structure of the model
# YOUR CODE HERE
nn_A1.summary()


# Compile the Sequential model
nn_A1.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])



# Fit the model using 50 epochs and the training data
fit_model_A1 = nn_A1.fit(X_train_scaled, y_train, epochs=50)






# Define the the number of inputs (features) to the model
number_input_features = len(X_train.iloc[0])

# Review the number of features
number_input_features


# Define the number of neurons in the output layer
number_output_neurons_A2 = 1


# Define the number of hidden nodes for the first hidden layer
hidden_nodes_layer1_A2 = 15# YOUR CODE HERE

# Review the number of hidden nodes in the first layer
hidden_nodes_layer1_A2


# Create the Sequential model instance
nn_A2 = Sequential()# YOUR CODE HERE


# First hidden layer
# YOUR CODE HERE
nn_A2.add(Dense(units=hidden_nodes_layer1_A2, input_dim=number_input_features, activation="relu"))
# Output layer
# YOUR CODE HERE
nn_A2.add(Dense(units=number_output_neurons_A2, activation="sigmoid"))

# Check the structure of the model
# YOUR CODE HERE
nn_A2.summary()


# Compile the model
# YOUR CODE HERE
nn_A2.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])


# Fit the model
# YOUR CODE HERE
fit_model_A2 = nn_A2.fit(X_train_scaled, y_train, epochs=50)






print("Original Model Results")

# Evaluate the model loss and accuracy metrics using the evaluate method and the test data
model_loss, model_accuracy = nn.evaluate(X_test_scaled, y_test, verbose=2)

# Display the model loss and accuracy results
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")


print("Alternative Model 1 Results")

# Evaluate the model loss and accuracy metrics using the evaluate method and the test data
model_loss, model_accuracy =nn_A1.evaluate(X_test_scaled, y_test, verbose=2)

# Display the model loss and accuracy results
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")


print("Alternative Model 2 Results")

# Evaluate the model loss and accuracy metrics using the evaluate method and the test data
model_loss, model_accuracy = nn_A2.evaluate(X_test_scaled, y_test, verbose=2)

# Display the model loss and accuracy results
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")





# Set the file path for the first alternative model
file_path_A1 = "AlphabetSoup_A1.h5"

# Export your model to a HDF5 file
# YOUR CODE HERE
nn_A1.save(file_path_A1)


# Set the file path for the second alternative model
file_path_A2 = "AlphabetSoup_A2.h5"

# Export your model to a HDF5 file
# YOUR CODE HERE
nn_A2.save(file_path_A2)



